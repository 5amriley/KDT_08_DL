{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor shape 변경\n",
    "- reshape(), view() : 원소 개수가 유지됨!, 기존 텐서 공유함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) 2\n"
     ]
    }
   ],
   "source": [
    "# 텐서 데이터 생성\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(t1.shape, t1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]) 3809040404736 3809040404736 3809040404760\n"
     ]
    }
   ],
   "source": [
    "print(t1, t1.data_ptr(), t1[0].data_ptr(), t1[1].data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3809040404736\n",
      "3809040404736\n",
      "3809040404736\n"
     ]
    }
   ],
   "source": [
    "# 메모리 저장 위치는 그대로.\n",
    "print(t1.reshape(1, 6).data_ptr())\n",
    "print(t1.reshape(3, 2).data_ptr())\n",
    "print(t1.reshape(6, 1).data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2x3 -> 3x2 형태 변경 : 원소 6개 동일\n",
    "# 메모리에서 한번에 읽어들이는 개수(단위) 변화 (메모리에서의 저장 순서는 동일함, Endian 동일)\n",
    "t1.view(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2x3 -> 6x1 형태 변경 : 원소 6개 동일\n",
    "t1.view(6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2x3 -> 6x? 형태 변경 : 원소 6개 동일\n",
    "# ? 는 컴파일러가 알맞게 지정하라는 뜻\n",
    "t1.view(6, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2x3 -> ?x1 형태 변경 : 원소 6개 동일\n",
    "# ? 는 컴파일러가 알맞게 지정하라는 뜻\n",
    "t1.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor.reshape()\n",
    "# view() 보다는 reshape() 사용을 권장. (메모리 상에서 새로운 위치에 만듦, 경우에 따라 deep copy)\n",
    "# Transpose 하면 원소의 순서가 달라지므로 deep copy.\n",
    "t1.reshape(6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원소 수가 유지되어야 하는데 그렇지 않으므로 오류 발생\n",
    "# t1.reshape(-1, 7) # 실제 원소는 6개인데, 7개를 표시하라니 오류 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전치 : 열 <-> 형 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(t1.shape)\n",
    "\n",
    "t2 = t1.T\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: view size is not compatible with input tensor's size and stride\n",
    "# t2.view(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4, 2, 5, 3, 6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4, 2, 5, 3, 6]]) True\n"
     ]
    }
   ],
   "source": [
    "t3 = t2.reshape(-1, 6)\n",
    "print(t3, t3.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 데이터의 메머리 저장 정보 즉 메타데이터\n",
    "- 현재 저장 형태, 검색 방향 정보, 시작 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) 2\n",
      "t1.storage() =>  1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]\n",
      "t1.storage_offset() => 0\n",
      "t1.stride() => (3, 1)\n",
      "t1.is_contiguous() => True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_28416\\2510581992.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f't1.storage() => {t1.storage()}')\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(t1.shape, t1.ndim)\n",
    "print(f't1.storage() => {t1.storage()}')\n",
    "print(f't1.storage_offset() => {t1.storage_offset()}')\n",
    "print(f't1.stride() => {t1.stride()}')  # (행이 바뀔 때 이동해야 하는 데이터 수, 열이 바뀔 때 이동해야 하는 데이터 수)\n",
    "print(f't1.is_contiguous() => {t1.is_contiguous()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) 2\n",
      "t2.storage() =>  1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]\n",
      "t2.storage_offset() => 0\n",
      "t2.stride() => (2, 1)\n",
      "t2.is_contiguous() => True\n"
     ]
    }
   ],
   "source": [
    "t2 = t1.view(-1, 2)\n",
    "print(t2.shape, t2.ndim)\n",
    "print(f't2.storage() => {t2.storage()}')\n",
    "print(f't2.storage_offset() => {t2.storage_offset()}')\n",
    "print(f't2.stride() => {t2.stride()}')\n",
    "print(f't2.is_contiguous() => {t2.is_contiguous()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 제거 / 추가\n",
    "- tensor.squeeze() : 텐서에서 차원이 1인 것 제거\n",
    "- tensor.unsqueeze(dim) : 텐서에 차원 1인 것 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "t1 = torch.tensor([[1, 2], [3, 4]])\n",
    "t2 = torch.tensor([[1, 2, 3, 4]])\n",
    "t3 = torch.tensor([[[1, 2, 3, 4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 => torch.Size([2, 2]), 2D, 3809040465920\n",
      "t2 => torch.Size([1, 4]), 2D, 3809040466048\n",
      "t3 => torch.Size([1, 1, 4]), 3D, 3809040466112\n"
     ]
    }
   ],
   "source": [
    "print(f't1 => {t1.shape}, {t1.ndim}D, {t1.data_ptr()}')\n",
    "print(f't2 => {t2.shape}, {t2.ndim}D, {t2.data_ptr()}')\n",
    "print(f't3 => {t3.shape}, {t3.ndim}D, {t3.data_ptr()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 차원 축소 -> torch.Size([2, 2]), 2D, 3809040465920\n",
      "t2 차원 축소 -> torch.Size([4]), 1D, 3809040466048\n",
      "t3 차원 축소 -> torch.Size([4]), 1D, 3809040466112\n"
     ]
    }
   ],
   "source": [
    "t11 = t1.squeeze()\n",
    "t22 = t2.squeeze()\n",
    "t33 = t3.squeeze()\n",
    "print(f't1 차원 축소 -> {t11.shape}, {t11.ndim}D, {t11.data_ptr()}')  # 차원이 1인 것이 없어서 줄지 않음\n",
    "print(f't2 차원 축소 -> {t22.shape}, {t22.ndim}D, {t22.data_ptr()}')\n",
    "print(f't3 차원 축소 -> {t33.shape}, {t33.ndim}D, {t33.data_ptr()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 차원 축소 -> torch.Size([1, 1, 4]), 3D, 3809040466112\n"
     ]
    }
   ],
   "source": [
    "t34 = t3.squeeze(dim=2)     # 차원 1일 경우 없앨 차원의 지정\n",
    "print(f't3 차원 축소 -> {t34.shape}, {t34.ndim}D, {t34.data_ptr()}')    # 2번째 차원이 4라서 squeeze 하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 정보 => torch.Size([2, 2]), 2D, 3809040465920\n"
     ]
    }
   ],
   "source": [
    "## 원소/요소 수 변경 없이 1 차원 증가시키가 => torch.unsqueeze(dim)\n",
    "print(f't1 정보 => {t11.shape}, {t11.ndim}D, {t11.data_ptr()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 차원 추가 => torch.Size([1, 2, 2]), 3D, 3809040465920, (4, 2, 1)\n",
      "t1 차원 추가 => torch.Size([2, 2, 1]), 3D, 3809040465920, (2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "t11 = t1.unsqueeze(dim=0)\n",
    "t22 = t1.unsqueeze(dim=-1)\n",
    "\n",
    "print(f't1 차원 추가 => {t11.shape}, {t11.ndim}D, {t11.data_ptr()}, {t11.stride()}')\n",
    "print(f't1 차원 추가 => {t22.shape}, {t22.ndim}D, {t22.data_ptr()}, {t22.stride()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[None].shape  # 길이가 1인 차원을 추가. (unsqueeze와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
